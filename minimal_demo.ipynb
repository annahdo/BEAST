{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import pandas\n",
    "from einops import repeat, rearrange\n",
    "import time\n",
    "\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_name = \"lmsys/vicuna-7b-v1.5\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16, use_cache=False, low_cpu_mem_usage=True).to(device).eval()\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_format = { 'user' : [\"USER: \", \"\"], 'assistant' : [\" ASSISTANT:\", \"\"], 'sep' : [\"\", \"\"]}\n",
    "# chat_format = { 'user' : [\"\", \"\"], 'assistant' : [\"\", \"\"], 'sep' : [\"\", \"\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "top_k = None # consider top_k tokens with highest probability\n",
    "top_p = 0.95 # consider tokens up to cummulative probability top_p\n",
    "k1 = 15 # k1: no. of candidates in beam \n",
    "k2 = 30 # k2: no. of candidates per candidate evaluated \n",
    "temperature = 1.0\n",
    "num_adv_tokens = 40 # number of adversarial tokens to be added to the input sequence\n",
    "lookahead_length = 10 # number of tokens in the target sequence to be considered in attack objective\n",
    "batch_size = 16\n",
    "data_file = \"data/harmful_behaviors.csv\"\n",
    "prompt_tag = \"goal\"\n",
    "target_tag = \"target\"\n",
    "num_samples = 5\n",
    "num_tokens_to_generate = 40\n",
    "begin = 1\n",
    "\n",
    "data = pandas.read_csv(\"data/harmful_behaviors.csv\")\n",
    "prompts = list(data[prompt_tag])[begin: begin + num_samples]\n",
    "targets = list(data[target_tag])[begin: begin + num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_prompts = prepare_input(chat_format, prompts)\n",
    "org_answers = generate(model, tokenizer, formated_prompts, max_new_tokens=num_tokens_to_generate, do_sample=True, temperature=temperature, top_p=top_p, top_k=top_k)\n",
    "\n",
    "for p, oa in zip(formated_prompts[:min(10, num_samples)], org_answers[:min(10, num_samples)]):\n",
    "    print(f\"Prompt: {p}\")\n",
    "    print(f\"Original answer: {oa}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_tokens = get_target_tokens(tokenizer, targets, lookahead_length).to(device)\n",
    "inputs = tokenizer(formated_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "\n",
    "inputs = generate_and_extend(model, tokenizer, inputs, max_new_tokens=1, do_sample=True, temperature=temperature, top_p=top_p, top_k=top_k, k=k1)\n",
    "\n",
    "for i in range(1, num_adv_tokens):\n",
    "        \n",
    "    inputs = generate_and_extend(model, tokenizer, inputs, max_new_tokens=1, do_sample=True, temperature=temperature, top_p=top_p, top_k=top_k, k=k2)\n",
    "    print(inputs['input_ids'].shape)\n",
    "    # extend input_ids with target tokens\n",
    "    total_tokens = {}\n",
    "    target_tokens_expanded = repeat(target_tokens['input_ids'], 'b t -> b r t', r=k1*k2)\n",
    "    target_tokens_expanded = rearrange(target_tokens_expanded, 'b r t -> (b r) t')\n",
    "    total_tokens['input_ids'] = torch.cat([inputs['input_ids'], target_tokens_expanded], dim=-1)\n",
    "    total_tokens['attention_mask'] = torch.cat([inputs['attention_mask'], torch.ones_like(target_tokens_expanded)], dim=-1)\n",
    "\n",
    "    # calculate perplexity\n",
    "    print('before perplexity')\n",
    "    time.sleep(5)\n",
    "    perplexity = calc_perplexity(total_tokens, model, batch_size=batch_size)\n",
    "    time.sleep(5)\n",
    "    print('after perplexity')\n",
    "\n",
    "    # choose the best candidate\n",
    "    inputs = choose_best_candidate(inputs, perplexity, best_k=k1, out_of=k1*k2, device=device)\n",
    "\n",
    "perplexity = calc_perplexity(inputs, model, batch_size=batch_size)\n",
    "final_attack = choose_best_candidate(inputs, perplexity, best_k=1, out_of=k1, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_attack_sentences = tokenizer.batch_decode(final_attack['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "attacked_answers = generate(model, tokenizer, final_attack_sentences, max_new_tokens=num_tokens_to_generate, do_sample=True, temperature=temperature, top_p=top_p, top_k=top_k)\n",
    "\n",
    "for p, oa, aa in zip(final_attack_sentences[:min(10, num_samples)], org_answers[:min(10, num_samples)], attacked_answers[:min(10, num_samples)]):\n",
    "    print(f\"attacked prompt: {p}\")\n",
    "    print(f\"original answer: {oa}\")\n",
    "    print(f\"attacked answer: {aa}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
